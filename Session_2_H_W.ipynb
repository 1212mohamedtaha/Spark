{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIwZsU0haNkj"
   },
   "source": [
    "## Task 1 - SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45wCuSvgajao"
   },
   "source": [
    "### Build SparkSession:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JwcbnqiymCE3"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession \n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8HITwTqnJcX"
   },
   "source": [
    "### Read the json file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "93iqAB7tnMYQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.json('DataFrames_sample.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNx0qMfunbKX"
   },
   "source": [
    "### Display the schema:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "UG4CcVJenc9y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- D: double (nullable = true)\n",
      " |-- H: double (nullable = true)\n",
      " |-- HDD: string (nullable = true)\n",
      " |-- Id: long (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- RAM: string (nullable = true)\n",
      " |-- ScreenSize: string (nullable = true)\n",
      " |-- W: double (nullable = true)\n",
      " |-- Weight: double (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "|   D|   H|      HDD| Id|      Model| RAM|ScreenSize|    W|Weight|Year|\n",
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "|9.48|0.61|512GB SSD|  1|MacBook Pro|16GB|       15\"|13.75|  4.02|2015|\n",
      "|7.74|0.52|256GB SSD|  2|    MacBook| 8GB|       12\"|11.04|  2.03|2016|\n",
      "|8.94|0.68|128GB SSD|  3|MacBook Air| 8GB|     13.3\"| 12.8|  2.96|2016|\n",
      "| 8.0|20.3|  1TB SSD|  4|       iMac|64GB|       27\"| 25.6|  20.8|2017|\n",
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zaj0nHTcngEF"
   },
   "source": [
    "### Get all the data when \"Model\" equal \"MacBook Pro\":\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Vm9QPKBCnkuS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "|   D|   H|      HDD| Id|      Model| RAM|ScreenSize|    W|Weight|Year|\n",
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "|9.48|0.61|512GB SSD|  1|MacBook Pro|16GB|       15\"|13.75|  4.02|2015|\n",
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('*').where('Model == \"MacBook Pro\"').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43oLte9LuGzA"
   },
   "source": [
    "### Create TempView:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "nVFYFcjtdIGW"
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('TempView')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCLMmjRLdjbT"
   },
   "source": [
    "### Display \"RAM\"column and count \"RAM\" column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "BxykutRjuF0X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|count(RAM)|\n",
      "+----------+\n",
      "|         4|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select count(RAM) from TempView ').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5nlwq4t9gvK"
   },
   "source": [
    "### Get all columns when \"Year\" column equal \"2015\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "WXxjFxN19hJl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "|   D|   H|      HDD| Id|      Model| RAM|ScreenSize|    W|Weight|Year|\n",
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "|9.48|0.61|512GB SSD|  1|MacBook Pro|16GB|       15\"|13.75|  4.02|2015|\n",
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from TempView  where Year = 2015').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHjK2Kqfuv24"
   },
   "source": [
    "### Get all when \"Model\" start with \"M\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "m30EkY_iu1Gs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "|   D|   H|      HDD| Id|      Model| RAM|ScreenSize|    W|Weight|Year|\n",
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "|9.48|0.61|512GB SSD|  1|MacBook Pro|16GB|       15\"|13.75|  4.02|2015|\n",
      "|7.74|0.52|256GB SSD|  2|    MacBook| 8GB|       12\"|11.04|  2.03|2016|\n",
      "|8.94|0.68|128GB SSD|  3|MacBook Air| 8GB|     13.3\"| 12.8|  2.96|2016|\n",
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from TempView  where Model LIKE \"M%\"').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Igw9iqJQ7TdH"
   },
   "source": [
    "### Get all data when \"Model\" column equal \"MacBook Pro\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "SRCGSB_W9cPc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "|   D|   H|      HDD| Id|      Model| RAM|ScreenSize|    W|Weight|Year|\n",
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "|9.48|0.61|512GB SSD|  1|MacBook Pro|16GB|       15\"|13.75|  4.02|2015|\n",
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from TempView  where Model = \"MacBook Pro\"').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZIlmJidw1Ep"
   },
   "source": [
    "### Get all data with Multiple Conditions when \"RAM\" column equal \"8GB\" and \"Model\" column is \"Macbook\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "-5T7roxgnBBV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+---+-------+---+----------+-----+------+----+\n",
      "|   D|   H|      HDD| Id|  Model|RAM|ScreenSize|    W|Weight|Year|\n",
      "+----+----+---------+---+-------+---+----------+-----+------+----+\n",
      "|7.74|0.52|256GB SSD|  2|MacBook|8GB|       12\"|11.04|  2.03|2016|\n",
      "+----+----+---------+---+-------+---+----------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from TempView  where Model = \"MacBook\" and RAM == \"8GB\"').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qk8YPAWQ8HxI"
   },
   "source": [
    "### Get all data with Multiple Conditions when \"D\" greater than or equal \"8\" and \"Model\" column is \"iMac\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "XDHJSpQK9MuS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+---+-----+----+----------+----+------+----+\n",
      "|  D|   H|    HDD| Id|Model| RAM|ScreenSize|   W|Weight|Year|\n",
      "+---+----+-------+---+-----+----+----------+----+------+----+\n",
      "|8.0|20.3|1TB SSD|  4| iMac|64GB|       27\"|25.6|  20.8|2017|\n",
      "+---+----+-------+---+-----+----+----------+----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from TempView  where D  >=8 and Model == \"iMac\" ').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5d6364f6"
   },
   "source": [
    "## Task 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlWhDvTPfZgu"
   },
   "source": [
    "### Read \"test1\" dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "7964d064"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.csv('test1.csv' , header=True)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJKjDOKHfnCt"
   },
   "source": [
    "### Display Salary of the people less than or equal to 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "c21edffc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|  Sunny| 29|         4| 20000|\n",
      "|   Paul| 24|         3| 20000|\n",
      "| Harsha| 21|         1| 15000|\n",
      "|Shubham| 23|         2| 18000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(\"*\").where('Salary <= 20000').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvFWNFJjf0Pq"
   },
   "source": [
    "### Display Salary of the people less than or equal to 20000 and greater than or equal 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "26f76ee1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|  Sunny| 29|         4| 20000|\n",
      "|   Paul| 24|         3| 20000|\n",
      "| Harsha| 21|         1| 15000|\n",
      "|Shubham| 23|         2| 18000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.where('(Salary <= 20000) and (Salary >= 15000)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VAcIXkTgN9D"
   },
   "source": [
    "## Task 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOeqRO2KgW34"
   },
   "source": [
    "### Read \"test3\" dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "4d3bd081"
   },
   "outputs": [],
   "source": [
    "df3 = spark.read.csv('test3.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejyUT1rngdeR"
   },
   "source": [
    "### Display dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "7ed791ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+\n",
      "|     Name| Departments|salary|\n",
      "+---------+------------+------+\n",
      "|    Krish|Data Science| 10000|\n",
      "|    Krish|         IOT|  5000|\n",
      "|   Mahesh|    Big Data|  4000|\n",
      "|    Krish|    Big Data|  4000|\n",
      "|   Mahesh|Data Science|  3000|\n",
      "|Sudhanshu|Data Science| 20000|\n",
      "|Sudhanshu|         IOT| 10000|\n",
      "|Sudhanshu|    Big Data|  5000|\n",
      "|    Sunny|Data Science| 10000|\n",
      "|    Sunny|    Big Data|  2000|\n",
      "+---------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xp42YtorghXJ"
   },
   "source": [
    "### Display schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "d57d24ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Departments: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "df3 = df3.withColumn(\"salary\", df3[\"salary\"].cast(IntegerType()))\n",
    "\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHxWeGCCgnww"
   },
   "source": [
    "### Group by \"Name\" column and using sum function on \"Name\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "f15f8197"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|     Name|sum(salary)|\n",
      "+---------+-----------+\n",
      "|Sudhanshu|      35000|\n",
      "|    Sunny|      12000|\n",
      "|    Krish|      19000|\n",
      "|   Mahesh|       7000|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df3.select('Name' , 'salary').groupBy('Name').sum().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWgkaU3bhUOL"
   },
   "source": [
    "### Group by \"Name\" column and using avg function on \"Name\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "fc122ace"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|     Name|       avg(salary)|\n",
      "+---------+------------------+\n",
      "|Sudhanshu|11666.666666666666|\n",
      "|    Sunny|            6000.0|\n",
      "|    Krish| 6333.333333333333|\n",
      "|   Mahesh|            3500.0|\n",
      "+---------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df3.select('Name' , 'salary').groupBy('Name').avg().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARg_-WPKhfL5"
   },
   "source": [
    "### Group by \"Departments\" column and using sum function on \"Departments\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "151d2264"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| Departments|sum(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|      15000|\n",
      "|    Big Data|      15000|\n",
      "|Data Science|      43000|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df3.groupBy('Departments').sum().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7rdLSEXhn4W"
   },
   "source": [
    "### Group by \"Departments\" column and using mean function on \"Departments\" column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "66fe5552"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| Departments|avg(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|     7500.0|\n",
      "|    Big Data|     3750.0|\n",
      "|Data Science|    10750.0|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.groupBy('Departments').mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bndivgGjhsbq"
   },
   "source": [
    "Group by \"Departments\" column and using count function on \"Departments\" column:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfPs99wnhwGu"
   },
   "source": [
    "### Apply agg to using sum function get the total of salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(salary)|\n",
      "+-----------+\n",
      "|      73000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.agg(sum(df3.salary)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(salary)|\n",
      "+-----------+\n",
      "|      73000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.agg({'salary':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(salary)|\n",
      "+-----------+\n",
      "|      73000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.agg(sum('salary')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYD0wGPRi1FO"
   },
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJLc1PY1i-Np"
   },
   "source": [
    "You've been flown to their headquarters in Ulsan, South Korea, to assist them in accurately estimating the number of crew members a ship will need.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PEoknoejL4r"
   },
   "source": [
    "They're currently building new ships for certain customers, and they'd like you to create a model and utilize it to estimate how many crew members the ships will require.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70slYH-tjR81"
   },
   "source": [
    "Metadata:\n",
    "1. Measurements of ship size \n",
    "2. capacity \n",
    "3. crew \n",
    "4. age for 158 cruise ships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZzrhGnHkRCU"
   },
   "source": [
    "It is saved in a csv file for you called \"ITI_data.csv\". our task is to develop a regression model that will assist in predicting the number of crew members required for future ships. The client also indicated that they have found that particular cruise lines will differ in acceptable crew counts, thus this is most likely an important factor to consider when conducting your investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "A9CZzWWqZnOC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+---+------------------+----------+------+------+-----------------+----+\n",
      "|  Ship_name|Cruise_line|Age|           Tonnage|passengers|length|cabins|passenger_density|crew|\n",
      "+-----------+-----------+---+------------------+----------+------+------+-----------------+----+\n",
      "|    Journey|    Azamara|  6|30.276999999999997|      6.94|  5.94|  3.55|            42.64|3.55|\n",
      "|      Quest|    Azamara|  6|30.276999999999997|      6.94|  5.94|  3.55|            42.64|3.55|\n",
      "|Celebration|   Carnival| 26|            47.262|     14.86|  7.22|  7.43|             31.8| 6.7|\n",
      "|   Conquest|   Carnival| 11|             110.0|     29.74|  9.53| 14.88|            36.99|19.1|\n",
      "|    Destiny|   Carnival| 17|           101.353|     26.42|  8.92| 13.21|            38.36|10.0|\n",
      "|    Ecstasy|   Carnival| 22|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|\n",
      "|    Elation|   Carnival| 15|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|\n",
      "|    Fantasy|   Carnival| 23|            70.367|     20.56|  8.55| 10.22|            34.23| 9.2|\n",
      "|Fascination|   Carnival| 19|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|\n",
      "|    Freedom|   Carnival|  6|110.23899999999999|      37.0|  9.51| 14.87|            29.79|11.5|\n",
      "|      Glory|   Carnival| 10|             110.0|     29.74|  9.51| 14.87|            36.99|11.6|\n",
      "|    Holiday|   Carnival| 28|            46.052|     14.52|  7.27|  7.26|            31.72| 6.6|\n",
      "|Imagination|   Carnival| 18|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|\n",
      "|Inspiration|   Carnival| 17|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|\n",
      "|     Legend|   Carnival| 11|              86.0|     21.24|  9.63| 10.62|            40.49| 9.3|\n",
      "|   Liberty*|   Carnival|  8|             110.0|     29.74|  9.51| 14.87|            36.99|11.6|\n",
      "|    Miracle|   Carnival|  9|              88.5|     21.24|  9.63| 10.62|            41.67|10.3|\n",
      "|   Paradise|   Carnival| 15|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|\n",
      "|      Pride|   Carnival| 12|              88.5|     21.24|  9.63| 11.62|            41.67| 9.3|\n",
      "|  Sensation|   Carnival| 20|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|\n",
      "+-----------+-----------+---+------------------+----------+------+------+-----------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5 = spark.read.csv(\"ITI_data.csv\", header = True, inferSchema=True)\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QTNLhZSlf9_"
   },
   "source": [
    "### OneHotEncoder \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "-ZZxxxKLZnOF"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer , OneHotEncoder, VectorAssembler\n",
    "df5 = df5.drop('Ship_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNCxWem0l662"
   },
   "source": [
    "###Use VectorAssembler to merge all columns into one column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+------------------+----------+------+------+-----------------+-----+\n",
      "|Cruise_line|Age|           Tonnage|passengers|length|cabins|passenger_density| crew|\n",
      "+-----------+---+------------------+----------+------+------+-----------------+-----+\n",
      "|    Azamara|  6|30.276999999999997|      6.94|  5.94|  3.55|            42.64| 3.55|\n",
      "|    Azamara|  6|30.276999999999997|      6.94|  5.94|  3.55|            42.64| 3.55|\n",
      "|   Carnival|  8|             110.0|     29.74|  9.51| 14.87|            36.99| 11.6|\n",
      "|   Carnival|  9|              88.5|     21.24|  9.63| 10.62|            41.67| 10.3|\n",
      "|   Carnival|  9|             110.0|     29.74|  9.52| 14.87|            36.99| 11.6|\n",
      "|   Carnival| 11|              86.0|     21.24|  9.63| 10.62|            40.49|  9.3|\n",
      "|   Carnival| 12|              88.5|     21.24|  9.63| 10.56|            41.67|10.29|\n",
      "|   Carnival| 12|              88.5|     21.24|  9.63| 11.62|            41.67|  9.3|\n",
      "|   Carnival| 13|           101.509|     27.58|  8.93| 13.79|            36.81| 11.5|\n",
      "|   Carnival| 14|           101.509|     27.58|  8.93| 13.21|            36.81| 10.0|\n",
      "|   Carnival| 15|            70.367|     20.52|  8.55|  10.2|            34.29|  9.2|\n",
      "|   Carnival| 17|            70.367|     20.52|  8.55|  10.2|            34.29|  9.2|\n",
      "|   Carnival| 17|           101.353|     26.42|  8.92| 13.21|            38.36| 10.0|\n",
      "|   Carnival| 18|            70.367|     20.52|  8.55|  10.2|            34.29|  9.2|\n",
      "|   Carnival| 19|            70.367|     20.52|  8.55|  10.2|            34.29|  9.2|\n",
      "|   Carnival| 22|            70.367|     20.52|  8.55|  10.2|            34.29|  9.2|\n",
      "|   Carnival| 23|            70.367|     20.56|  8.55| 10.22|            34.23|  9.2|\n",
      "|   Carnival| 26|            47.262|     14.86|  7.22|  7.43|             31.8|  6.7|\n",
      "|  Celebrity|  5|             122.0|      28.5| 10.33|  6.87|            34.57|  6.7|\n",
      "|  Celebrity| 11|              91.0|     20.32|  9.65|  9.75|            44.78| 9.99|\n",
      "+-----------+---+------------------+----------+------+------+-----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF , testDF = df5.randomSplit([0.8,0.2] , seed  =42)\n",
    "trainDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "pE4ohNjVZnOG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#In the following section:\n",
    "strIndexer = StringIndexer(\n",
    "    inputCol='Cruise_line',\n",
    "    outputCol='Cruise_line_indexed',handleInvalid='skip') \n",
    "\n",
    "\n",
    "#Replace the StringIndexer with OneHotEncoder as follows:\n",
    "ohtEncoder = OneHotEncoder(dropLast=False,inputCol='Cruise_line_indexed',\n",
    "            outputCol='Cruise_line_encoded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tonnage', 'passengers', 'length', 'cabins', 'passenger_density']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numColumns = [f for (f,dt) in trainDF.dtypes if ((dt=='double')&(f != 'crew'))]\n",
    "numColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cruise_line_encoded',\n",
       " 'Tonnage',\n",
       " 'passengers',\n",
       " 'length',\n",
       " 'cabins',\n",
       " 'passenger_density']"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assemblerInputs = ['Cruise_line_encoded'] + numColumns\n",
    "assemblerInputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecAssembler = VectorAssembler(inputCols=assemblerInputs, outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rbf56f6AmUUl"
   },
   "source": [
    "### Create a Linear Regression Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "id": "nvqnqTkunkNx"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr =  LinearRegression(labelCol='crew' , featuresCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVdxQTcSC6Cz"
   },
   "source": [
    "### Creating a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "id": "UqM9HxkNIHwE"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[strIndexer, ohtEncoder,  vecAssembler , lr ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/28 17:22:42 WARN Instrumentation: [dc41cfbb] regParam is zero, which might cause numerical instability and overfitting.\n",
      "21/10/28 17:22:42 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "21/10/28 17:22:42 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "21/10/28 17:22:42 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "21/10/28 17:22:42 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n",
      "21/10/28 17:22:42 WARN Instrumentation: [dc41cfbb] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n"
     ]
    }
   ],
   "source": [
    "pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "predDF = pipelineModel.transform(testDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEbfwhqeHOCc"
   },
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "regressionEvaluator = RegressionEvaluator(predictionCol='prediction', labelCol='crew', metricName='rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.581945799710334"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse = regressionEvaluator.evaluate(predDF)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8507705383879058"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "regressionEvaluator = RegressionEvaluator(predictionCol='prediction', labelCol='crew', metricName='r2')\n",
    "r2 = regressionEvaluator.evaluate(predDF)\n",
    "r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SluXM6j-Gt9B"
   },
   "source": [
    "By Eng. Mostafa Nabieh \n",
    "If you have questions, please feel free to ask.\n",
    "\n",
    "My Email : nabieh.mostafa@yahoo.com\n",
    "\n",
    "My Whatsapp : +201015197566"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Session 2 H.W.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
